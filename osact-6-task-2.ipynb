{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSACT 6 Task 2: Translating Arabic Dialects to Modern Standard Arabic\n",
    "\n",
    "Among the challenges involved with using Arabic in large language models is the disparity of dialects used throughout the Arab world. While modern Standard Arabic is used prolifically in official and government use-cases, among the general public, it is seldom ever used. Instead, various **vernaculars** are used, local to various regions across the Middle East and North Africa. These vernaculars can be divided into subgroups, the count of which can vary from the single digits to dozens, depending on the level of granularity through which one defines the geographic differences.\n",
    "\n",
    "\n",
    "<br>\n",
    "<center> <img src = \"./Arabic_Dialects.svg.png\" width=60%> \n",
    "\n",
    "<p style=\"font-size: 9px; width: 60%\" >\n",
    "Schmitt, Genevieve A. (2019). \"Relevance of Arabic Dialects: A Brief Discussion\". In Brunn, Stanley D.; Kehrein, Roland (eds.). Handbook of the Changing World Language Map. Springer. p. 1385. doi:10.1007/978-3-030-02438-3_79. ISBN 978-3-030-02437-6. as \"Fig. 1 Major dialects of Arabic, by region. (Open source)\".\n",
    "</p>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The dataset provided by OSACT-6 aggregates the dialects into the following five language subgroups:\n",
    "    \n",
    "- Egyptian\n",
    "- Iraqi\n",
    "- Levantine\n",
    "- Maghrebi\n",
    "- Gulf\n",
    "\n",
    "The dataset contains 200 sentences from each dialect, with its corresponding translation in MSA. As such, the baseline naive model accuracy the machine learning model should be expected to beat is 0.2.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<div style=\"background-color: #0055aa; color: white; width: 50%; padding: 0.5em; border-radius: 1em\">\n",
    "<b>Note:</b> On the 14 of February, the competition updated to include revisions of the Gulf arabic sentences.\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "required_packages = ['matplotlib', 'wordcloud', 'scikit-learn', 'nltk', 'arabic_reshaper', 'python-bidi', 'transformers', 'pandas']\n",
    "\n",
    "for p in required_packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading datasets\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"./datasets\"):\n",
    "    print(\"downloading datasets\")\n",
    "    zipfile.ZipFile(io.BytesIO(requests.get(\"https://codalab.lisn.upsaclay.fr/my/datasets/download/951b472f-f58d-4831-a233-e3757ccc4fa7\").content)).extractall(\"datasets\")\n",
    "    zipfile.ZipFile(io.BytesIO(requests.get(\"https://codalab.lisn.upsaclay.fr/my/datasets/download/2eb2e7fd-e259-409f-a968-efe7a8fb528b\").content)).extractall(\"datasets\")\n",
    "else:\n",
    "    print(\"dataset directory exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## EDA\n",
    "\n",
    "### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_json(\"./dev_set_all.json\")\n",
    "dataset = dataset.set_index(\"id\")\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dialect.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts and Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(\"dialect\").sum()[[\"source\"]]\n",
    "\n",
    "dataset[\"source_char_length\"] = dataset[\"source\"].str.len()\n",
    "dataset[\"source_word_length\"] = dataset[\"source\"].str.split().str.len()\n",
    "dataset[\"target_char_length\"] = dataset[\"target\"].str.len()\n",
    "dataset[\"target_word_length\"] = dataset[\"target\"].str.split().str.len()\n",
    "\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(\"dialect\").sum().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset.groupby(\"dialect\").sum()[[\"source_word_length\"]].plot(kind=\"bar\", xlabel=\"Dialect\", ylabel=\"\", legend=\"\", title=\"Total word count of each dialect in the dataset\")\n",
    "\n",
    "dataset.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset[[\"dialect\", \"source_word_length\"]].groupby(\"dialect\").sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As word counts are completely unrelated to the type of dialect, this data may bias the model, creating an association between length and dialect.\n",
    "\n",
    "### Term Frequency Analysis\n",
    "\n",
    "We can get a better understanding of how the model should perform by examining the differences between dialects and MSA. Each MSA target is unique, suggesting that each source-target pair is unique in meaning. This complicates the analysis, but it should be doable given the appropriate approach. To begin, we can explore each of the regional dialects.\n",
    "\n",
    "#### Unigram Analysis\n",
    "\n",
    "We observe the corpuses both with and without stopwords, to compare and better understand the differences in regional dialects. We first begin with a unigram analysis, using wordclouds. The code below constructs two sets of wordclouds. Each set contains six wordclouds, one for each of the five dialects, and MSA. One set contains the stopwords, the other does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud as wc\n",
    "from arabic_reshaper import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# This is going to run terribly but it'll do for now\n",
    "def generate_wordcloud(text, width = 1500, height = 1500):\n",
    "    render_ready_text = get_display(arabic_reshaper.reshape(text))\n",
    "    return wc.WordCloud(\n",
    "        font_path = \"./Janna-LT-Bold.ttf\",\n",
    "        background_color = \"white\",\n",
    "        colormap=\"Blues\",\n",
    "        width = width,\n",
    "        height = height,\n",
    "    ).generate(render_ready_text)\n",
    "\n",
    "\n",
    "clouds_without_stopwords = []\n",
    "clouds_with_stopwords = []\n",
    "\n",
    "for dialect in dataset.dialect.unique():\n",
    "    raw_text = dataset[dataset[\"dialect\"] == dialect][\"source\"].sum()\n",
    "\n",
    "    filtered_text = \" \".join([w for w in raw_text.split() if w not in stopwords.words(\"arabic\")])\n",
    "    render_ready_filtered_text = get_display(arabic_reshaper.reshape(filtered_text)) \n",
    "   \n",
    "    clouds_without_stopwords.append(generate_wordcloud(raw_text))\n",
    "    clouds_with_stopwords.append(generate_wordcloud(filtered_text))\n",
    "\n",
    "#Adding the MSA WordClouds\n",
    "msa_text = dataset[\"target\"].sample(200).sum()\n",
    "filtered_msa_text = \" \".join([w for w in msa_text.split() if w not in stopwords.words(\"arabic\")])\n",
    "\n",
    "clouds_without_stopwords.append(generate_wordcloud(filtered_msa_text))\n",
    "clouds_with_stopwords.append(generate_wordcloud(msa_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3)\n",
    "\n",
    "fig.set_dpi(250)\n",
    "\n",
    "fig.suptitle(\"Most Common Words, Not Including Stopwords\")\n",
    "\n",
    "axes[0, 0].set_title(\"Egyptian\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "axes[0, 0].imshow(clouds_without_stopwords[0])\n",
    "\n",
    "axes[0, 1].set_title(\"Iraqi\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "axes[0, 1].imshow(clouds_without_stopwords[1])\n",
    "\n",
    "axes[0, 2].set_title(\"Levantine\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "axes[0, 2].imshow(clouds_without_stopwords[2])\n",
    "\n",
    "axes[1, 0].set_title(\"Magharebi\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "axes[1, 0].imshow(clouds_without_stopwords[3])\n",
    "\n",
    "axes[1, 1].set_title(\"Gulf\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "axes[1, 1].imshow(clouds_without_stopwords[4])\n",
    "\n",
    "axes[1, 2].set_title(\"MSA\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "axes[1, 2].imshow(clouds_without_stopwords[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3)\n",
    "\n",
    "fig.set_dpi(250)\n",
    "\n",
    "fig.suptitle(\"Most Common Words, Including Stopwords\")\n",
    "\n",
    "axes[0, 0].set_title(\"Egyptian\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "axes[0, 0].imshow(clouds_with_stopwords[0])\n",
    "\n",
    "axes[0, 1].set_title(\"Iraqi\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "axes[0, 1].imshow(clouds_with_stopwords[1])\n",
    "\n",
    "axes[0, 2].set_title(\"Levantine\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "axes[0, 2].imshow(clouds_with_stopwords[2])\n",
    "\n",
    "axes[1, 0].set_title(\"Magharebi\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "axes[1, 0].imshow(clouds_with_stopwords[3])\n",
    "\n",
    "axes[1, 1].set_title(\"Gulf\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "axes[1, 1].imshow(clouds_with_stopwords[4])\n",
    "\n",
    "axes[1, 2].set_title(\"MSA\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "axes[1, 2].imshow(clouds_with_stopwords[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a preliminary glance, it appears that the stopwords provide enough variance to the dataset that it would be best to include them- they provide big hints as to what dialect the sentence comes from. Still, we will attempt to generate text using both corpuses as training data to see which provides the most accurate translations. We also note similarities between Iraqi and Levantine arabic, which tracks with the linguistic theories of arabic - Iraqi Arabic is also commonly referred to as South Levantine Arabic.\n",
    "\n",
    "#### Bi-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngrams = {}\n",
    "for d in dataset.dialect:\n",
    "    vec = CountVectorizer(stop_words=stopwords.words(\"arabic\"), ngram_range=(2,2))\n",
    "    bow = vec.fit_transform(dataset.groupby(\"dialect\").sum().loc[d][[\"source\"]])\n",
    "    count_values = bow.toarray().sum(axis=0)\n",
    "\n",
    "    ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in vec.vocabulary_.items()], reverse = True))\n",
    "    ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "\n",
    "    ngram_freq = ngram_freq.head(10)\n",
    "    ngram_freq[\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x)))\n",
    "    ngrams.update({d: ngram_freq})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3)\n",
    "fig.subplots_adjust(hspace=1.5, wspace=0.5)\n",
    "fig.set_dpi(150)\n",
    "\n",
    "fig.suptitle(\"Bigram Frequency, Not including stopwords\")\n",
    "\n",
    "axes[0, 0].set_title(\"Egyptian\")\n",
    "axes[0, 0].bar(ngrams[\"Egyptian\"][\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x))), ngrams[\"Egyptian\"][\"frequency\"])\n",
    "axes[0, 0].tick_params(rotation=90)\n",
    "\n",
    "axes[0, 1].set_title(\"Iraqi\")\n",
    "axes[0, 1].bar(ngrams[\"Iraqi\"][\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x))), ngrams[\"Iraqi\"][\"frequency\"])\n",
    "axes[0, 1].tick_params(rotation=90)\n",
    "\n",
    "axes[0, 2].set_title(\"Levantine\")\n",
    "axes[0, 2].bar(ngrams[\"Levantine\"][\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x))), ngrams[\"Levantine\"][\"frequency\"])\n",
    "axes[0, 2].tick_params(rotation=90)\n",
    "\n",
    "axes[1, 0].set_title(\"Magharebi\")\n",
    "axes[1, 0].bar(ngrams[\"Magharebi\"][\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x))), ngrams[\"Magharebi\"][\"frequency\"])\n",
    "axes[1, 0].tick_params(rotation=90)\n",
    "\n",
    "axes[1, 1].set_title(\"Gulf\")\n",
    "axes[1, 1].bar(ngrams[\"Gulf\"][\"ngram\"].apply(lambda x: get_display(arabic_reshaper.reshape(x))), ngrams[\"Gulf\"][\"frequency\"])\n",
    "axes[1, 1].tick_params(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style=\"background-color: #0055aa; color: white; width: 50%; padding: 0.5em; border-radius: 1em\">\n",
    "<b>Note:</b> matplotlib appears to be unable to render unicode character (U+FDF2), ï·², rendering it as a square.\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "### LDA Analysis\n",
    "\n",
    "To better understand the distinction and crossover between dialects statistically, It may be worth projecting dialects to a 2D plane and searching for clusters. First, we encode the corpus in tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "categories = np.array(dataset.dialect)\n",
    "\n",
    "sources = dataset[\"source\"].to_list()\n",
    "target = dataset.dialect\n",
    "\n",
    "\n",
    "def construct_lda_components(stopwords_flag = \"off\"):\n",
    "    args = {\n",
    "        \"min_df\": 2,\n",
    "        #\"max_df\": 0.8,\n",
    "        #\"max_features\": 4000\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**args) if stopwords_flag == \"off\" else TfidfVectorizer( stop_words =stopwords.words(\"arabic\"), **args)\n",
    "    tfidf = vectorizer.fit_transform(sources)\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=2, solver=\"svd\")\n",
    "    ldaComponents = lda.fit(tfidf.toarray(), target)\n",
    "    ldaComponents = lda.transform(tfidf.toarray())\n",
    "    return ldaComponents\n",
    "\n",
    "\n",
    "def scatter_plot_from_components(ax, ldaComponents, title):\n",
    "    for dialect, color in zip(set(target), ['#800000', '#0040a0', '#4B0082', '#B8860B', '#556B2F']):\n",
    "        ax.scatter(ldaComponents[target == dialect, 0], ldaComponents[target == dialect, 1], c=color, label=dialect)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "scatter_plot_from_components(axs[0], construct_lda_components(\"off\"), \"LDA of Corpus Including Stopwords\")\n",
    "scatter_plot_from_components(axs[1], construct_lda_components(\"on\"), \"LDA of Corpus Excluding Stopwords\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides some interesting insights into the structure and vocabulary of the various dialects of the language. Our earlier hypothesis is correct - the stopwords function as independent variables in the dataset, and so should be kept for classification purposes. Without them, the vocabularies of Iraqi, Gulf, and Levantine arabic are very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "categories = np.array(dataset.dialect)\n",
    "\n",
    "final_set = pd.read_json(\"./test_set_all.public.json\").sample(1500)\n",
    "\n",
    "sources = final_set[\"source\"].to_list()\n",
    "target = final_set.dialect\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "scatter_plot_from_components(axs[0], construct_lda_components(\"off\"), \"LDA of Corpus Including Stopwords\")\n",
    "scatter_plot_from_components(axs[1], construct_lda_components(\"on\"), \"LDA of Corpus Excluding Stopwords\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset, however, tells a different story. In this case, it may be best to have three models: One dedicated to Iraqi, One to maghrebi, and another for the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_set.info())\n",
    "print(final_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "There are two main approaches I'm considering:\n",
    "\n",
    "- Training the entire dataset on a single model\n",
    "- Using a classification algorithm the sentence to a dialect, then run it through a model trained specifically on said dialect - perhaps an MoE as a more sophisticated method?\n",
    "- translate dialects to an intermediate form before final translation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
